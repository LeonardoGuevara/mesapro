# Human Detection System and Robot Safety System for Thorvald robots

This repository contains a ROS package that allows the Thorvald rbotos to detect and safely interact with humans during logistic operations at polytunnels. The package includes a human detection system based on 2D LiDARs, RGB-D, and thermal cameras. The human information extracted from these sensors (humans position, motion, orientation, gesture) is used as input for a safety system that can perform safety actions such as 1) modify the robot's current goal (the goals are nodes in a topological map of the field) according to human commands (i.e. activated by gesture recognition),  2) controlling/modulating the robot velocities according to gesture recognition and/or the distance between the robot and human (when the robot is moving towards the human) 3) stop completely any operation when a human is detected within unsafe distance respect to the robot (in the case of UV-C treatment a safety distance is 7m, in the case of logistics it is 1.2m), 4) Activate audiovisual alerts in order to warn the human about the robot's current action and potential danger (critical during UV-C treatment). The components and overall system architecture are shown in the following scheme (if you click on the image it will direct you to a demo video).

[![Overal_system](/Human_perception_and_safety_system_new.png)](https://www.youtube.com/watch?v=vIdlauwlmKo)

# HOW THE HUMAN DETECTION WORKS:
* The human detection system is based on information taken from two 2D LiDARs (the same used for robot localization), two RGBD cameras (realsense D455) one aligned to the robot's local x-axis (to detect a human in front of the robot), and the other one in the opposite direction (to detect a human in the back of the robot). Moreover, a thermal camera (FLIR Lepton 3.5) was mounted together with each RBGD camera using a 3D printed base in order to match the images extracted from both cameras.
* To start publishing human information, the detection system requires to be subscribed to topics published by 2D LiDARs nodes or RGBD cameras nodes, or both in the best case (for sensor fusion). When only LiDAR data is available, the human information contains only (x,y) position, a label with the motion estimation (if the human is static or not static), and the number of the area around the robot in which the human is being detected (there is a total of 10 areas around the robot, 5 correspond to the front of the robot and 5 to the back). When the RGBD data is available, then apart from the information above, the detection system delivers a human orientation label (if the human is facing or not the robot), a human posture label (if the human is performing a specific body gesture). If Thermal cameras are also publishing data (it is not mandatory), then, this information is used to robustify the human detection ( based only on RBD images) and remove false positives.  The thermal information is also valuable for UV-C treatment scenarios when even if a human skeleton is not detected, but If a certain percentage of the pixel on the thermal image is higher than a threshold, then a thermal detection label is activated to alert the safety system that the robot must pause operation.
* The name of the labels corresponding to the human body gestures that can be interpreted by the robot are: "no_gesture","left_arm_up","left_hand_front","left_arm_sideways","left_forearm_sideways","right_arm_up","right_hand_front","right_arm_sideways","right_forearm_sideways","both_arms_up","both_hands_front","crouched_down","picking".
* The name of the labels corresponding to the human motion are: "not_defined","mostly_static", "moving".
* The name of the labels corresponding to the human orientation are: "facing_the_robot", "giving_the_back", "left_side", "right_side".
* The following figures illustrate the distribution of the areas around the robot (used for sensor fusion and safety purposes) and show samples of the body gestures mentioned above.

# HOW THE DECISION MAKING WORKS:
* The decision-making controls the behavior of the safety system based on safety policies (determined during a Hazard Analysis stage) and information delivered by the human detection system and the Thorvald navigation system.
* The safety system must always be publishing messages, even if no safety action is required. If the safety topics stop being published for a specific period of time (e.g. if the safety system node stopped suddenly), the decision-making makes the current robot action stop and activates audiovisual alerts to warn the human know that the safety system is not running. When the safety system starts publishing again, the previous robot action is resumed.
* Similar to the safety system, the human detection system is always publishing messages, even if no human is detected. Thus, If human information is not being published for a while, the safety system makes the current robot action stop and activates audiovisual alerts to warn the human that the robot perception is not running. The robot can resume the previous action only when the human detection system is running again.
* If teleoperation mode is activated, then, any robot's autonomous action is stopped (as the standard behavior of Thorvald robots) and audiovisual alerts indicate to the human that the robot is being controlled by the joystick. When the autonomous mode is activated again, the previous action is not resumed and the robot keeps on pause, till the human gives a new command/goal.
* The information being published by the safety system include safety action label, voice message label, human command label, risk level label, operation mode label, action mode label, topological goal label, critical human detected index. 
* The name of the labels corresponding to the safety actions published by the safety system are: "move_to_goal","approach_to_human","move_away_from_human","pause","wait_for_new_human_command","teleoperation","gesture_control","no_safety_action".
* The name of the labels corresponding to the audio/voice messages are: "no_message","alert_UVC_danger","ask_for_next_action","ask_for_free_space","alert_approaching","alert_moving_away","alert_moving_to_goal","safety_system_error","human_perception_error","teleoperation_mode","gesture_control_mode".
* The name of the labels corresponding to human commands performed by body gesture are:
"no_command","approach","move_away","stop","move_forwards","move_backwards","move_right","move_left".
* The name of the labels corresponding to the level of risk during the Human-Robot Interaction (HRI) are:
"no_human","safety_hri","risky_hri","dangerous_hri".
* The operation mode labels can be only "logistics" or "UV-C_treatment". Depending on which operation is selected, the safety system behavior will be different since the safety policies for each application are different.
* The action mode labels can be "polytunnel" or "footpath". They are updated depending if the robot is navigating outside the polytunnel or along a row inside the polytunnel. If the action mode is "polytunnel" it means that the human gestures are limited to commands that make the robot stop, approach him/her, or make the robot move away. If the action mode is "footpaths" it means that the human gestures can also control the robot's motion in any direction outside the polytunnel, including moving sideways. The robot action activated by a gesture outside the polytunnel is valid only while the gesture is being detected. On the other hand, any robot action activated by a gesture inside the polytunnel is still valid after the human stop performing that gesture.
* The topological goal can be set as the standard Thorlvald navigation, i.e. by using rviz interface (clicking on a node). However, when the safety system is being used, this goal can be modified in some situations. For instance, if the robot is navigating in "polytunnel" mode, and the human performs gestures to make the robot move away or approach him/her, then the robot's goal is updated in order to make the robot move in the proper direction according to the human command.
* The human critical index corresponds to the index of the element in the human detected list which represent the most critical human to be tracked. This is critical when more than one human is detected at the same time. The selection of the most critical human to be tracked depends on the area in which the humans are located, the distance between them and the robot, and if they are performing body gestures or not.

SAFETY POLICIES:
In order to minimize the risk of getting human injuries during HRIs, the following safety policies were considered for the decision-making:


# HOW TO USE THE MESAPRO PACKAGE:

* To run the human detection and safety system on the Thorvald robots. You have to launch the config files into the folder "tmule". There are 3 config files that must be launched in order to run everything shown on the system architecture scheme. If implementing it on the Thorvald-014 (the one used during the whole MeSAPro project), the rasberry-hri_navigation.yaml is launched on the NUC (computer without GPU, used as master), the rasberry-hri_safety_perception.yaml is launched on the ZOTAC (computer with GPU), and the rasberry-hri_monitoring.yaml can be launched in any laptop in order to visualize and monitor the robot localization, human detections and safety actions.
* The config files have several parameters that can be modified in case some features of the human detection or safety system are not required for certain tests. For instance, the leg detection can be disabled to test only camera detections, the thermal information can be disable, audio or visual alerts can be disabled if neccesary, etc.
* To test the safety system in simulation (GAZEBO), you can launch the config file rasberry-hri_sim.yaml .
* To test the human detection system (based only on camera data) using bag files, you can launch the config file rasberry-hri_camera_detector.yaml .
* To test the human detection system (based only on LiDAR data) using bag files, you can launch the config file rasberry-hri_leg_detector.yaml .

# NOTES: 
* The creation of this package was motivated by the MeSAPro project which aims to ensure autonomy of agricultural robots in scenarios that involve human-robot interactions. The decision-making and safety policies of the current version of the safety system are designed to be implemented mainly during logistics operations at polytunnels, but can also be used during UV-C treatment operations.
* To use the mesapro package, it is required to also have all the packages into the LCAS/RASberry repository which includes among others, the topological navigation package which is used to make the Thorvald robots navigate in an autonomous way at polytunnels (Note: The RASberry repository is private, so make sure you have access to it).
* To launch any config file into the tmule folder, it is necesary to install "tmule".
